{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "288d1bda2a6a4ff19087c2d52bf47f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4cc2c8b37a04d6f98b8d253e4db010c",
              "IPY_MODEL_11b60b3ad716439cba11b4a7c7e2b966",
              "IPY_MODEL_3e1fedce0a164fb7aff9e4c7bc984bb3"
            ],
            "layout": "IPY_MODEL_6a73229de21d4be181ca4a320020acaa"
          }
        },
        "b4cc2c8b37a04d6f98b8d253e4db010c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a06de8498154259a4bddcbe43637d8e",
            "placeholder": "​",
            "style": "IPY_MODEL_106a155f6f96416f9e4bd639659f1232",
            "value": "llama-2-13b-chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "11b60b3ad716439cba11b4a7c7e2b966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_587b2b88054346c48dda885a17b52e74",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47aedf4fb5b84418aa47742b0e13b8da",
            "value": 9763701888
          }
        },
        "3e1fedce0a164fb7aff9e4c7bc984bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed0fd38ffd584d068814e4d83075a1e0",
            "placeholder": "​",
            "style": "IPY_MODEL_0df69e31b19d4658b445d41e57d8a766",
            "value": " 9.76G/9.76G [02:12&lt;00:00, 74.5MB/s]"
          }
        },
        "6a73229de21d4be181ca4a320020acaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a06de8498154259a4bddcbe43637d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106a155f6f96416f9e4bd639659f1232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "587b2b88054346c48dda885a17b52e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47aedf4fb5b84418aa47742b0e13b8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed0fd38ffd584d068814e4d83075a1e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df69e31b19d4658b445d41e57d8a766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Freitus/pln-atividades/blob/main/2023_Q3_PLN_ATIVIDADE_PR%C3%81TICA_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 05 [LangChain + Grandes Modelos de Linguagem + API]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 05** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/C1oUi1FKTZ4W9fNdA\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia **10/12 (domingo)** APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "Renan Oliveira da Silva 11201810188\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "Alex Freitas De Lisboa 11100816\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VbYD2mw8y4CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada equipe deve selecionar um Grande Modelo de Linguagem (*Large Language Model - LMM*). Preferencialmente, usar um modelo gratuito. Cada modelo pode ser escolhido por até 4 equipes.\n",
        "\n",
        ">\n",
        "\n",
        "Uma lista de LLMs está disponível em:\n",
        "\n",
        "> https://integrations.langchain.com/llms\n",
        "> https://python.langchain.com/docs/integrations/llms/"
      ],
      "metadata": {
        "id": "_UlblxFxzDV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelos da Hugging Face:**\n",
        "\n",
        "* Mistral Base: modelo promissor.\n",
        "\n",
        "> https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\n",
        "\n",
        "* Mistral Quantizado: mais leve. Pode ser um pouco mais difícil de configurar, mas deve ocupar menos memória.\n",
        "\n",
        "> https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "\n",
        "* Mistral Lite da Amazon: pode ter desempenho melhor.\n",
        "\n",
        "> https://huggingface.co/amazon/MistralLite\n",
        "\n",
        "* Llama 2 7B: modelo da Meta melhorado.\n",
        "\n",
        "> https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "* Llama 2 7b 32k: contexto maior pelo mesmo tamanho.\n",
        "\n",
        "> https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct\n",
        "\n",
        "* Galactica: para artigos científicos\n",
        "\n",
        "> https://huggingface.co/facebook/galactica-6.7b\n",
        "\n",
        "* Alpaca: alternativo ao Llama\n",
        "\n",
        "> https://huggingface.co/chavinlo/alpaca-native\n",
        "\n",
        "> https://huggingface.co/spaces/tloen/alpaca-lora"
      ],
      "metadata": {
        "id": "GVpiVhzn3QqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lista de Modelos Interessantes:**\n",
        "\n",
        "> https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html\n",
        "\n",
        "* Vicuna:\n",
        "> https://huggingface.co/lmsys/vicuna-7b-v1.5-16k\n",
        "\n",
        "* Openchat kit:\n",
        "> https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
        "\n",
        "* Meta OPT:\n",
        "> https://huggingface.co/facebook/opt-1.3b\n",
        "\n",
        "* Google Flan:\n",
        "> https://huggingface.co/google/flan-t5-base"
      ],
      "metadata": {
        "id": "WRuAiSr05Ayt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APIs:**\n",
        "\n",
        "* GPT4ALL: tenta ser um chatGPT aberto\n",
        "> https://python.langchain.com/docs/integrations/llms/gpt4all\n",
        "\n",
        "* YandexGPT: da empresa russa que criou um modelo de *Machine Learning* clássico muito bom, o CatBoost.\n",
        "> https://python.langchain.com/docs/integrations/llms/yandex\n",
        "\n",
        "* Anthropic: empresa forte concorrente da OpenAi.\n",
        "> https://python.langchain.com/docs/integrations/chat/anthropic\n",
        "\n",
        "* Gorilla: pipeline para geração de código\n",
        " > https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html"
      ],
      "metadata": {
        "id": "5Q3mhqda5WP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo esse levantamento e comentários foram feitos pelo aluno  **Bruno Sanches Rodrigues**."
      ],
      "metadata": {
        "id": "DO6XMjHx58u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados do LLM selecionada:\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**LLM**:\n",
        "\n",
        ">\n",
        "\n",
        "**Link para a documentação oficial**:\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**Site oficial (GitHub)**:"
      ],
      "metadata": {
        "id": "a6AkE6iW0c3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não pode ser o modelo da `OpenAI`."
      ],
      "metadata": {
        "id": "A2oo8GtK0xSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por favor, informe os dados da API selecionada:\n",
        "\n",
        "**API**:\n",
        "\n",
        "**Site oficial**:\n",
        "\n",
        "**Link para a documentação oficial**:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: não é necessário usar a mesma **API** da `ATIVIDADE PRÁTICA 03`. Cada **API** pode ser usada por até 4 equipes."
      ],
      "metadata": {
        "id": "bTODq98Myt_u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso do framework **`LangChain`** (obrigatório) e de um **LLM** aplicando, no mínimo, uma técnica de PLN. A técnica pode ser aplicada em qualquer córpus. Também é obrigatório usar uma **API** da `ATIVIDADE PRÁTICA 03`. A **API** pode ser usada tanto para obter os dados quanto para disponibilizar os resultados.\n",
        "\n",
        "O **LLM** e a **API** selecionados devem ser informados na seguinte planilha:\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1cOL7zVNffqmliuv23zFm1UJjhEXTdp1Zm5EyAJmhiKg/edit?usp=sharing\n",
        "\n",
        ">\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   Similaridade de Textos\n",
        "*   Reconhecimento de Entidades Nomeadas\n",
        "*   Sistemas de Perguntas e Respostas\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Lista de APIs:**\n",
        "\n",
        "\n",
        "* YouTube\n",
        "* LinkedIn\n",
        "* Twitter (X)\n",
        "* Facebook\n",
        "* Instagram\n",
        "* Medium\n",
        "* Reddit\n",
        "* TikTok\n",
        "* GitHub\n",
        "* Pinterest\n",
        "* Telegram\n",
        "* Dados financeiros\n",
        "* Notícias\n",
        "* Mercado de Ações\n",
        "* Dados financeiros\n",
        "* SMS\n",
        "* OpenAlex\n",
        "* Whisper (OpenAI)\n",
        "* Discord\n",
        "* Slack\n",
        "* Chuck Norris Jokes\n",
        "* Wikipedia\n",
        "* Last.fm\n",
        "* New York Times\n",
        "* Nasdaq Data Link\n",
        "* Yahoo! Finance\n",
        "* Twilio SendGrid Mail Send\n",
        "* Spotify\n",
        "* Awesome API\n",
        "* Google Books API\n",
        "* Mercado Livre API\n",
        "\n",
        ">\n",
        "\n",
        "**PLANILHA DA ATIVIDADE PRÁTICA 03:**\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1-Q1szJ3UmoE2_3LtcRQyqid5fPIcnpsR3XAPnoxLj2o/edit?usp=sharing\n",
        "\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação os segunintes pontos:\n",
        "\n",
        "* Uso do framework **`LangChain`**.\n",
        "\n",
        "* Escolha e uso de um **LLM**.\n",
        "\n",
        "* Escolha e uso de uma **API**\n",
        "\n",
        "* Criatividade no uso do framework **`LangChain`** em conjunto com o **LLM** e a **API**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: todo o código do notebook deve ser executado. Código sem execução não será considerado."
      ],
      "metadata": {
        "id": "LhwdrMp123Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.5 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "!pip install huggingface_hub\n",
        "!pip install llama-cpp-python==0.1.78\n",
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "fYf0nrGGGzht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "5dkhzlSMMSSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n"
      ],
      "metadata": {
        "id": "29FrLS96MXq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "CbfxJSC-MZ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "hSJQ7gwvf-Wk",
        "outputId": "1fd522e8-7925-4bc8-d8ba-d831da4dd3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "288d1bda2a6a4ff19087c2d52bf47f28",
            "b4cc2c8b37a04d6f98b8d253e4db010c",
            "11b60b3ad716439cba11b4a7c7e2b966",
            "3e1fedce0a164fb7aff9e4c7bc984bb3",
            "6a73229de21d4be181ca4a320020acaa",
            "9a06de8498154259a4bddcbe43637d8e",
            "106a155f6f96416f9e4bd639659f1232",
            "587b2b88054346c48dda885a17b52e74",
            "47aedf4fb5b84418aa47742b0e13b8da",
            "ed0fd38ffd584d068814e4d83075a1e0",
            "0df69e31b19d4658b445d41e57d8a766"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "288d1bda2a6a4ff19087c2d52bf47f28"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # núcleos de CPU\n",
        "    n_batch=512, # Deve estar entre 1 e n_ctx, considere a quantidade de VRAM em sua GPU.\n",
        "    n_gpu_layers=32 # Altere esse valor com base no seu modelo e no pool VRAM da GPU.\n",
        "    )"
      ],
      "metadata": {
        "id": "j0reu36wgQx6",
        "outputId": "0a5afaf7-a996-493b-bae5-494a52e987c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Veja o número de camadas na GPU\n",
        "lcpp_llm.params.n_gpu_layers"
      ],
      "metadata": {
        "id": "8kOk3qsngtvb",
        "outputId": "20eec25c-ce87-408a-ac29-94853809148e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Escreva uma função em python que de o resultado de 20 dividido por 10\"\n",
        "prompt_template=f'''SYSTEM: Você é um assistente pessoal que busca ajudar o melhor possível.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "QvB3si36g4Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=lcpp_llm(prompt=prompt_template, max_tokens=256, temperature=0.3, top_p=0.95,\n",
        "                  repeat_penalty=1.2, top_k=150,\n",
        "                  echo=True)"
      ],
      "metadata": {
        "id": "3VmmQSQWhvdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "lfg6aucTh4H3",
        "outputId": "e5ada637-7479-41b5-cf35-b5ac30ffb6d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: Você é um assistente pessoal que busca ajudar o melhor possível.\n",
            "\n",
            "USER: Escreva uma função em python que de o resultado de 20 dividido por 10\n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "Com certeza! Aqui está a função em Python para calcular o resultado de 20 dividido por 10:\n",
            "```\n",
            "def divide(numero, divisor):\n",
            "    return numero / divisor\n",
            "\n",
            "resultado = divide(20, 10)\n",
            "print(resultado)\n",
            "```\n",
            "Espero que isso ajude! Se você tiver alguma outra pergunta, não hesite em me perguntar. Estou aqui para ajudar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade python-telegram-bot"
      ],
      "metadata": {
        "id": "HmbzTG9npW8i",
        "outputId": "b70f18ac-8d8f-4736-a450-9cdf3d27af97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-telegram-bot\n",
            "  Downloading python_telegram_bot-20.7-py3-none-any.whl (552 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.6/552.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx~=0.25.2 (from python-telegram-bot)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.25.2->python-telegram-bot) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx~=0.25.2->python-telegram-bot) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx~=0.25.2->python-telegram-bot)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx~=0.25.2->python-telegram-bot) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.25.2->python-telegram-bot) (1.3.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx~=0.25.2->python-telegram-bot)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx~=0.25.2->python-telegram-bot) (1.2.0)\n",
            "Installing collected packages: h11, httpcore, httpx, python-telegram-bot\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 python-telegram-bot-20.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyTelegramBotAPI"
      ],
      "metadata": {
        "id": "DffUUgedp1-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import telegram\n",
        "from telegram.ext import Updater, CommandHandler\n",
        "\n",
        "# Defina o token do seu bot\n",
        "TOKEN = 'AAHG99qGFOEaCW5wfykUFfMhqZrvaT28LSY'\n",
        "\n",
        "def start(update, context):\n",
        "    # Obtém o nome do usuário\n",
        "    user_name = update.message.from_user.first_name\n",
        "    mensagem = f\"Olá {user_name}!\"\n",
        "\n",
        "    # Envia a mensagem de resposta\n",
        "    context.bot.send_message(chat_id=update.effective_chat.id, text=mensagem)\n",
        "\n",
        "def main():\n",
        "    # Cria o objeto Updater e passa o token do bot\n",
        "    updater = Updater(token=TOKEN, use_context=True)\n",
        "\n",
        "    # Obtém o despachante para registrar os manipuladores de comandos\n",
        "    dispatcher = updater.dispatcher\n",
        "\n",
        "    # Registra o manipulador de comando para o comando \"/start\"\n",
        "    dispatcher.add_handler(CommandHandler(\"start\", start))\n",
        "\n",
        "    # Inicia o bot\n",
        "    updater.start_polling()\n",
        "\n",
        "    # Mantém o bot em execução até que seja interrompido\n",
        "    updater.idle()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Q_MhoXhQnoGD",
        "outputId": "b919a6c4-0202-4b09-82d1-7e6a67fb55ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8e1727f9057e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtelegram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUpdater\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCommandHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMessageHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Token de acesso do bot do Telegram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'6808014166:AAHG99qGFOEaCW5wfykUFfMhqZrvaT28LSY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Filters' from 'telegram.ext' (/usr/local/lib/python3.10/dist-packages/telegram/ext/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#o código deve aplicar uma função que consiste em o usuário entrar em contato via telegram, CHATBOT, onde o mesmo enviaria uma pergunta ou um pedido, como no exemplo acima: \"Escreva uma função em python que de o resultado de 20 dividido por 10\"\n",
        "#o ChatBot armazena essa informação em um prompt e repassa para o LLM que gera uma resposta e retransmite ao ChatBot, gerando uma resposta ao usuário.\n",
        "#basicamente fazendo com que o Telegram vire um \"ChatGPT\"."
      ],
      "metadata": {
        "id": "WlqMRKxh-d5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}