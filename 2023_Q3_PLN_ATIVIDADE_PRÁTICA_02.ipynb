{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D7hJlilKM485"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Freitus/pln-atividade-02/blob/main/2023_Q3_PLN_ATIVIDADE_PR%C3%81TICA_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 02 [Extração e Pré-processamento de Dados + Expressões Regulares]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 02** deve ser feita utilizando o **Google Colab** com uma conta\n",
        "sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/83JggUJ1mhgWviEaA\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 16/10 (segunda-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "Renan Oliveira da Silva - 11201810188\n"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: `Capitulo 8\n",
        "\n",
        "`Segundo capítulo:` Capitulo 18\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` para identificar ERROS em 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        "Os capítulos devem ser selecionados na seguinte planilha:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**DICA:** Por favor, insira o seu nome ou da sua equipe na ordem definida na planilha. Por exemplo, se a linha correspondente ao o GRUPO 5 já foi preenchida, a próxima equipe (GRUPO 6) deverá ser informada na próxima linha da planilha.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TIPOS DE ERROS**\n",
        "---\n"
      ],
      "metadata": {
        "id": "eD_AJQhrwJQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: consulta feita no ChatGPT\n",
        ">\n",
        "\n",
        "Um `programa Python` que utilize `expressões regulares` pode ajudar a identificar vários **tipos de erros** comuns em **livros**, especialmente erros de formatação e problemas relacionados à consistência do texto. Aqui estão alguns exemplos de erros comuns que podem ser identificados usando expressões regulares:\n",
        "\n",
        "* Erros de gramática e ortografia: erros de digitação, concordância verbal e nominal, uso incorreto de pontuação e outros erros gramaticais.\n",
        "\n",
        "* Problemas de formatação: você pode usar expressões regulares para encontrar erros de formatação, como espaços em excesso, tabulações inadequadas ou alinhamentos inconsistentes.\n",
        "\n",
        "* Abreviações e acrônimos: você pode usar expressões regulares para encontrar abreviações ou acrônimos que não foram definidos ou explicados anteriormente no texto.\n",
        "\n",
        "* Citações e referências: expressões regulares podem ser úteis para localizar citações ou referências que precisam de formatação especial.\n",
        "\n",
        "* OUTROS TIPOS DE ERROS: não considerem apenas os tipos de erros citados acima.\n",
        "\n",
        "\n",
        "**IMPORTANTE:** Lembre-se de que expressões regulares podem ser poderosas, mas também complexas. Dependendo da complexidade dos erros que você deseja identificar, pode ser necessário ajustar as expressões regulares de acordo com as características específicas do seu texto. Além disso, é importante ter em mente que as expressões regulares podem não ser a melhor ferramenta para todos os tipos de erros em livros, especialmente problemas mais contextuais ou semânticos, que podem exigir abordagens de PLN mais avançadas.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gz0DTI0KYmn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A equipe que **realizar mais testes** e/ou **identificar mais erros** terá o peso diminuido na AVALIAÇÃO (Prova Escrita) em **25%** (caindo de 40 para 30). Os testes e possíveis erros devem ser contabizados de maneira separada.\n",
        "\n",
        ">\n",
        "\n",
        "Além disso, **por se tratar de um livro**, há um teste importante que deve ser feito. Lembre-se que o teste deve ser feito utilizando expressões regulares. A equipe que realizar esse teste, mesmo que o erro não ocorra nos capítulos selecionados, terá o peso diminuido na AVALIAÇÃO (Prova Escrita) em **25%** (caindo de 40 para 30).\n",
        "\n",
        "> A equipe pode considerar outros capítulos do livro para tentar identificar esse tipo de erro.\n",
        "\n",
        "**Se for a mesma equipe, o peso da avaliação será reduzido em 50% (caindo de 40 para 20)**.\n",
        "\n",
        ">\n",
        "\n",
        "**IMPORTANTE**: a diminuição no peso da AVALIAÇÃO será aplicado para todos os membros da equipe. Esse critério será aplicado apenas para uma equipe, considerando como critério de desempate a equipe que entregar primeiro a atividade no formulário.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo_Se6Lcy5pV",
        "outputId": "93ca342e-2812-407f-c59d-519ff99b7799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('machado')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnnneOJyze5D",
        "outputId": "41f8f2fa-f75f-4a82-a48e-cc76c53a2da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# Baixar o recurso \"punkt\" do NLTK\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkmTBel9zqlC",
        "outputId": "88a77c12-acc4-43f7-9f64-4fbfe55b5f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def remover_html(texto):\n",
        "    soup = BeautifulSoup(texto, \"html.parser\")\n",
        "    texto_limpo = soup.get_text()\n",
        "    return texto_limpo\n",
        "\n",
        "def identificar_erros(texto):\n",
        "    # Remover as marcações HTML do texto\n",
        "    texto_limpo = remover_html(texto)\n",
        "\n",
        "    # Tokenizar o texto em sentenças\n",
        "    sentencas = nltk.sent_tokenize(texto_limpo)\n",
        "\n",
        "    # Tokenizar cada sentença em palavras e identificar os erros\n",
        "    erros = []\n",
        "    for sentenca in sentencas:\n",
        "        palavras = nltk.word_tokenize(sentenca)\n",
        "        for palavra in palavras:\n",
        "            if not re.match(r'^[A-Za-zÀ-ú]+$', palavra):\n",
        "                continue  # Ignorar palavras com caracteres especiais\n",
        "            if not palavra.isalpha():\n",
        "                erros.append(palavra)\n",
        "\n",
        "    return erros"
      ],
      "metadata": {
        "id": "NVzu84O848fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def contar_palavras(texto):\n",
        "    palavras = nltk.word_tokenize(texto)\n",
        "    return len(palavras)"
      ],
      "metadata": {
        "id": "fsN_4I22Kxvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def verificar_link(url):\n",
        "    response = requests.head(url)\n",
        "    if response.status_code == 200:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "YhBeljl1MrMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verificar_texto_vazio(texto):\n",
        "    if not texto.strip():\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "LBhZeVtSMsnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verificar_erros_ortograficos(erros):\n",
        "    if len(erros) == 0:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "nUQL4Z3eMtsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verificar_termo(texto, termo):\n",
        "    if termo in texto:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "T7rkAY4EMu9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_paragrafos(texto):\n",
        "    paragrafos = [p.strip() for p in texto.split(\"\\n\\n\") if p.strip()]\n",
        "    return len(paragrafos)"
      ],
      "metadata": {
        "id": "cDPT7aMuPvae"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verificar_duplicatas(texto):\n",
        "    palavras = nltk.word_tokenize(texto)\n",
        "    palavras_sem_pontuacao = [palavra.lower() for palavra in palavras if palavra.isalpha()]\n",
        "    palavras_unicas = set(palavras_sem_pontuacao)\n",
        "    duplicatas = [palavra for palavra in palavras_unicas if palavras_sem_pontuacao.count(palavra) > 1]\n",
        "    return len(duplicatas)"
      ],
      "metadata": {
        "id": "efdj42k88VYw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verificar_caixa_alta_baixa(texto):\n",
        "    palavras = re.findall(r'\\b\\w+\\b', texto)\n",
        "    palavras_unicas = {}\n",
        "    for palavra in palavras:\n",
        "        palavra_minuscula = palavra.lower()\n",
        "        if palavra_minuscula not in palavras_unicas:\n",
        "            palavras_unicas[palavra_minuscula] = palavra\n",
        "        elif palavras_unicas[palavra_minuscula].lower() == palavra_minuscula:\n",
        "            # Verifica se a palavra já foi encontrada em minúsculas antes\n",
        "            palavras_unicas[palavra_minuscula] = palavra\n",
        "    # Retorna apenas as palavras em que a versão maiúscula e minúscula são diferentes\n",
        "    palavras_caixa_alta_baixa = [palavras_unicas[palavra] for palavra in palavras_unicas if palavra != palavras_unicas[palavra].lower()]\n",
        "    return list(palavras_caixa_alta_baixa)"
      ],
      "metadata": {
        "id": "pWg0sPsx_NPc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def top_3_palavras(texto):\n",
        "    # Remover tags HTML do texto\n",
        "    texto_sem_html = re.sub('<[^>]+>', '', texto)\n",
        "\n",
        "    # Remover caracteres não alfabéticos e converter para minúsculas\n",
        "    texto_limpo = re.sub('[^a-zA-Zá-úÁ-Ú]', ' ', texto_sem_html).lower()\n",
        "\n",
        "    # Dividir o texto em palavras\n",
        "    palavras = texto_limpo.split()\n",
        "\n",
        "    # Palavras a serem excluídas (por exemplo, artigos, preposições)\n",
        "    palavras_excluidas = set(['a', 'o', 'e', 'em', 'de', 'para', 'com'])\n",
        "\n",
        "    # Contar a ocorrência de cada palavra\n",
        "    contador = Counter(palavras)\n",
        "\n",
        "    # Excluir palavras indesejadas\n",
        "    palavras_filtradas = [(palavra, frequencia) for palavra, frequencia in contador.items()\n",
        "                          if palavra not in palavras_excluidas and len(palavra) > 1]\n",
        "\n",
        "    # Ordenar palavras por frequência\n",
        "    palavras_ordenadas = sorted(palavras_filtradas, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Obter as 3 palavras mais frequentes\n",
        "    top_3 = palavras_ordenadas[:3]\n",
        "\n",
        "    return top_3"
      ],
      "metadata": {
        "id": "IKfwua4mSB_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solicitar ao usuário que insira o link do texto\n",
        "url = input(\"Insira o link do capítulo do livro:\")\n",
        "\n",
        "# Solicitar ao usuário que insira uma frase para verificar no texto\n",
        "frase = input(\"Insira uma palavra ou frase para verificar se há no capítulo:\")\n",
        "\n",
        "# Verificar se o link é válido\n",
        "if not verificar_link(url):\n",
        "    print(f\"O link {url} não é válido.\")\n",
        "else:\n",
        "    # Obter o conteúdo do link\n",
        "    response = requests.get(url)\n",
        "    texto_html = response.text\n",
        "\n",
        "    # Verificar se o texto está vazio\n",
        "    if verificar_texto_vazio(texto_html):\n",
        "        print(f\"O capítulo {url} está vazio.\")\n",
        "    else:\n",
        "        # Remover as marcações HTML do texto\n",
        "        texto_limpo = remover_html(texto_html)\n",
        "\n",
        "        # Identificar os erros ortográficos\n",
        "        erros = identificar_erros(texto_limpo)\n",
        "\n",
        "        # Verificar se existem erros ortográficos\n",
        "        if verificar_erros_ortograficos(erros):\n",
        "            print(f\"Nenhum erro ortográfico foi encontrado a princípio no capítulo.\")\n",
        "\n",
        "        # Verificar se existem duplicatas no texto\n",
        "        duplicatas = verificar_duplicatas(texto_limpo)\n",
        "        if duplicatas > 0:\n",
        "          print(f\"Foram encontradas {duplicatas} duplicatas no texto.\")\n",
        "        else:\n",
        "          print(f\"Não foram encontradas duplicatas no texto.\")\n",
        "\n",
        "        # Verificar se existem palavras que aparecem em diferentes formas, maiúscula e minúscula.\n",
        "        duplicatas = verificar_caixa_alta_baixa(texto_limpo)\n",
        "        if len(duplicatas) > 0:\n",
        "          print(f\"Foram encontradas {len(duplicatas)} palavras sendo escritas em diferentes formas, maiúscula e minúscula.\")\n",
        "        else:\n",
        "          print(f\"Não foram encontradas palavras sendo escritas em diferentes formas, maiúscula e minúscula.\")\n",
        "\n",
        "        # Contar as palavras do texto\n",
        "        quantidade_palavras = contar_palavras(texto_limpo)\n",
        "        print(f\"O capítulo possui {quantidade_palavras} palavras.\")\n",
        "\n",
        "        # Contar os parágrafos do texto\n",
        "        quantidade_paragrafos = contar_paragrafos(texto_limpo)\n",
        "        print(f\"O capítulo possui {quantidade_paragrafos} parágrafos.\")\n",
        "\n",
        "        # Obter o top 3 das palavras mais frequentes\n",
        "        top_palavras = top_3_palavras(texto_limpo)\n",
        "\n",
        "        # Exibir o top 3\n",
        "        print(\"Top 3 palavras mais frequentes:\")\n",
        "        for palavra, frequencia in top_palavras:\n",
        "            print(f\"{palavra}: {frequencia}\")\n",
        "        # Verificar se a frase está presente no texto\n",
        "        if verificar_termo(texto_limpo, frase):\n",
        "            print(f\"A frase '{frase}' está presente no capítulo.\")\n",
        "        else:\n",
        "            print(f\"A frase '{frase}' não está presente no capítulo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh4hpTX_Mv62",
        "outputId": "542cf5b3-82e3-4bcd-e3b3-fdcdf9e2f8e0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insira o link do capítulo do livro:https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap18/cap18.html\n",
            "Insira uma palavra ou frase para verificar se há no capítulo:sai\n",
            "Nenhum erro ortográfico foi encontrado a princípio no capítulo.\n",
            "Foram encontradas 1091 duplicatas no texto.\n",
            "[]\n",
            "Não foram encontradas palavras duplicatas sendo escritas em caixa alta e baixa no texto.\n",
            "O capítulo possui 18981 palavras.\n",
            "O capítulo possui 236 parágrafos.\n",
            "Top 3 palavras mais frequentes:\n",
            "traduã: 262\n",
            "da: 236\n",
            "que: 202\n",
            "A frase 'sai' está presente no capítulo.\n"
          ]
        }
      ]
    }
  ]
}